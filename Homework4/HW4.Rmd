---
title: "Tarea 4. Estadística Aplicada 3"
output: html_notebook
---
```{r setup, include=FALSE, results=FALSE}
#Instalar las librerias necesarias y cargarlas
instalar <- function(paquete) {
    if (!require(paquete,character.only = TRUE,
                 quietly = TRUE, 
                 warn.conflicts = FALSE)){
          install.packages(as.character(paquete),
                           dependecies = TRUE,
                           repos = "http://cran.us.r-project.org")
          library(paquete,
                  character.only = TRUE, 
                  quietly = TRUE,
                  warn.conflicts = FALSE)
    }
}
<<<<<<< Updated upstream
libs <- c('rrcov', 'nortest', 'corrplot', 'CCA', 'candisc') 
=======
libs <- c( 'HistData', 'rrcov', 'nortest', 'corrplot', 'CCA', 
'candisc','car','dummies','corrplot','knitr','ggplot2', 'reshape2', 'MASS') 
>>>>>>> Stashed changes
lapply(libs, instalar)
```


##Ejercicio 3
Determina de la muestra sus variables canónicas y sus correlaciones. Interpreta las cantidades. Son las primeras variables canonicas un buen resumen de sus conjuntos de variables?


```{r}
R.3<-cbind(c(1106.0, 396.7, 108.4, .787, 26.230), c(396.7, 2382.0, 1143.0, -.214, -23.96),c(108.4,1143.0,2136.0,2.189,-20.84), c(.787, -.214, 2.189, .016, .216), c(26.23, -23.960, -20.84, .216, 70.56) )
R11.3<-R.3[4:5,4:5];R22.3<-R.3[1:3,1:3];R12.3<-R.3[4:5,1:3];R21.3<-t(R12.3)
A.3 <- solve(R11.3) %*% R12.3 %*% solve(R22.3) %*% R21.3
sA.3 <- eigen(A.3);sA.3
```

El primer eigenvalor es $\lambda_1 = 0.26764579$ y entonces la correlación entre las dos primeras variables canónicas es $\sqrt{\lambda_1} =  0.5173449$. La combinación lineal correspondiente a las variables del grupo de estructura es
           a'y = 0.99 ∗ peso_relativo − 0.0024 ∗ glucosa_plasmática_en_ayunas 

```{r}
# Cálculo de la matriz B
B.3 <- solve(R22.3) %*% R21.3 %*% solve(R11.3) %*% R12.3
sB.3 <- eigen(B.3);sB.3
```

La correspondiente combinación lineal es:
b´x = 0.43 ∗ int_glucosa - 0.47 ∗ resp_in_gluoral + 0.768 ∗ins_resis

La variabilidad explicada por las primeras variables canónicas es 100 $X$ $\frac{\lambda_1}{\sum{\lambda_i}}=$94%, entonces las primeras variables canonicas son un buen resumen de sus conjuntos de variables.

La primer variable canónica le da un peso mucho mayor al peso relativo que a la glucosa en ayunas. Mientras que, la seguda le da más pedo a la respuesta de glucosa oral que a la intolerancia a la glucosa y resistencia a la insulina. 

La variable canónica sobre medicina le da más peso al resistencia de insulida. Entre más resistencia hay, mayor es la variable canónica.

##Ejercicio 6
Objetivo: determinar si las variables fisiológicas se relacionan de alguna forma con las
variables de ejercicio.
a.Analizar la matriz de correlaciones relevantes entre las variables de los dos grupos.
b.Probar la hipótesis H0 : Σxy = 0

```{r}
data.6<-read.table("FitnessClubData.dat", header = T)
```

Analizar la matriz de correlaciones relevantes entre las variables de los dos grupos

```{r}
(R.6<-cor(data.6))
```

```{r}
corrplot(R.6)
```

Parece que hay una correlación alta entre sentadillas y la medición de la cintura. También parecen correlacionadas las sentadillas con el peso y las lagartijas con la medida de la cintura.

```{r}
cor(data.6[,1:3], data.6[,4:6])
```

Las correlaciones entre las variables fisiológicas y de ejercicio son moderadas, la más grande es de -0.65 entre cintura y sentadillas. Hay correlaciones más grandes dentro del conjunto: 0.8702 entre peso y cintura, 0.6957 entre mentones y abdominales, y 0.6692 entre abdominales y saltos.

Probar $H0: cov(x,y)=0$

```{r}
(cca <- stats::cancor(x = data.6[,1:3], y = data.6[,4:6]))
```

Las correlaciones entre las dos funciones canónicas, son 0.8, 0.2 y 0.73 respectivamente. Entonces, la primera correlación canónica de 0.7956 es mayor que cualquiera de las correlaciones entre conjuntos. 

```{r}
(cc <- cancor(x = data.6[,1:3], y = data.6[,4:6]))
```

Ahora, para probar la hipótesis necesitamos hacer una prueba. Tenemos con Wilks lambda con p-value de 0.06353>0.05, entonces no rechazamos la hipótesis nula. Por lo que no rechazamos la hipótesis nula de que no hay relación entre los dos conjuntos de variables y concluimos que los dos conjuntos de variables son independientes.

<<<<<<< Updated upstream
=======
## Ejercicio 7

### a) Usano diferentes simbolos para cada grupo, plotea los pares de datos $(x_1,x_2)$ $(x_1,x_3)$ y $(x_1,x_4)$ ¿Parece que los datos son aproximados a una normal bivariada para cualquiera de estos pares?

### b) Usa $n_1 = 21$ pares de observaciones $(x_1,x_2)$



## Ejercicio 8: GPA & GMAT

### A) Calcular las medias muestrales y la varianza agrupada

```{r}
#pooled variance

#functions
ni_s <- function(x, population){
  
  pis <- unique(x[,population])
  ni <- c()
  for(pi in pis) 
    ni <- c(ni,length(which(x[,population] == pi)))
  return(ni)
}

#variance
Si_s <- function(x, population){
  Si <- list()
  pis <- unique(x[,population])
  

  for(pi in pis) {
    Si[[pi]] <- cov(x[x[,population] == pi,-population])

  }
  return(Si)
  
}

Spool <-  function(df, population = 3){
  #number of populations
  m <- length(unique(df[,population]))

  #size ni
  ni_vec <- ni_s(df, population)

  #individual variance
  Si_list <- Si_s(df, population)
  
  #pool variance
  #upper part of the fraction
  Spool <- matrix(0,nrow = 2,ncol = 2)

  for(i in 1:m)
    Spool <- Spool +  (ni_vec[i] - 1)[1]  *Si_list[[i]] 
  
  #lower part (constant)
  Spool <- Spool / sum(ni_vec -1)
  
  return(Spool)
}


#read data
df_r <- read.table('T11-6.DAT')
n <- dim(df_r)[1]


#means and pooled variance
# mean
means <- list()
for(i in 1:3) means[[i]] <- colMeans(df_r[ df_r[,3] == i ,c(1,2)])
means

#pooled variance
Sp <- Spool(df_r)

#print means
# knitr::kable(means, col.names = 'population_mean')

cat("\n La varianza agrupada es: ")
Sp



```



### $W$ y $B$, discriminante generalizado. 



```{r}
sample_bet_g <- function(means, mean_o){
  B <- matrix(0, nrow = 2, ncol = 2)
  for(xbar in means){
    B <- B + (xbar - mean_o) %*% t(xbar - mean_o)
  }
  return(B)
}



#overal mean
g <- length(unique(df_r[,3]))         #num of groups
mean_o <- c(0,0)
for(xbar in means)
  mean_o <- mean_o + xbar             #overal mean of groups

# B
B <- sample_bet_g(means, mean_o)

# W
W <- (n - g)* Sp
Winv <- solve(W)
# eigen W-1B
sWB <-eigen(Winv%*% B)

B
Winv
sWB
```

Clasificaremos el siguiente alumno $x_{0}^{T} = [3.21 \quad 497]$.

```{r}
#new obs
x.9 <- c(3.21, 497)

# discriminants

#first we make unite variance 
v1 <- t(as.matrix(sWB$vectors[,1])) %*% Sp %*% as.matrix(sWB$vectors[,1])
v2 <- t(as.matrix(sWB$vectors[,2])) %*% Sp %*% as.matrix(sWB$vectors[,2])

a1 <- as.matrix(sWB$vectors[,1]) / v1[1]
a2 <- as.matrix(sWB$vectors[,2]) / v2[1]

# discriminants
y <- t(cbind(a1,a2)) %*% x.9
mu_y_p1 <- t(cbind(a1,a2)) %*% means[[1]]
mu_y_p2 <- t(cbind(a1,a2)) %*% means[[2]]
mu_y_p3 <- t(cbind(a1,a2)) %*% means[[3]]


#clasiffy
score_p1 <- t(y - mu_y_p1) %*% (y - mu_y_p1)
score_p2 <- t(y - mu_y_p2) %*% (y - mu_y_p2)
score_p3 <- t(y - mu_y_p3) %*% (y - mu_y_p3)

score_p1
score_p2
score_p3

```

Vemos que el score más pequeño es el de la población $\prod_{3}$ , por lo que _clasificamos_ al estudiante nuevo $x_0$ como alguien en el _border line_ .

La clasificación _sí_ coincide con la del libro.

## Ejercicio 9

```{r}
data("hemophilia")
plot(hemophilia$AHFactivity)
plot(hemophilia$AHFantigen)
```

a. Investiga si son datos normales bivariados

```{r}
ad.test(hemophilia$AHFactivity)
ad.test(hemophilia$AHFantigen)
```
Con una significancia alta, tenemos que se prueba la hipótesis de normalidad bivariada.

b. Obtener la función lineal muestral distriminante 

Recordar que el análisis discriminante lineal hace exactamente lo mismo que correlación
canónica.

```{r}
(mc.9 <- CovMcd(hemophilia[,1:2]))
col <- ifelse(hemophilia$gr == "carrier", 2, 3) 
plot(mc.9, which="tolEllipsePlot", class=TRUE, col=col)
plot(AHFantigen~AHFactivity, data=hemophilia, col=as.numeric(as.factor(gr))+1)

```

La gráfica anterior indica que es apropiado hacer un análisis discriminante lineal. En este diagrama, los puntos verdes representan a las sanas, mientras que los portadores de hemofilia se representan con puntos de color rojo.

```{r}
datos.9 <- melt(hemophilia, value.name = "valor")
aggregate(formula = valor ~ gr + variable, data = datos.9,  FUN = function(x){shapiro.test(x)$p.value})
```


Todos tienen p-value mayor a 0.05, entonces no se rechaza y decimos que son todos normales.

%%Análisis discriminante lineal

b. Función discriminante lineal de asumiendo probabilidades iguales

```{r}
modelo_lda.9 <- lda(formula = gr ~ AHFactivity + AHFantigen, data = hemophilia)
modelo_lda.9
```
c. Usando la función anterior clasificar las nuevas observaciones

```{r}
nuevas_observaciones <- data.frame(AHFactivity = c(-.112,-.059,.064,-.043,-.05,-.094,-.123,-.011,-.21,-.126), AHFantigen = c(-.279,-.068,.012,-.052,-.098,-.113,-.143,-.037,-.09,-.019) )
```


```{r}
predict(object = modelo_lda.9, newdata = nuevas_observaciones)
```


```{r}
predicciones <- predict(object = modelo_lda.9, newdata = hemophilia[, c(1,2)], method = "predictive")
table(hemophilia$gr, predicciones$class, dnn = c("Clase real", "Clase predicha"))
```

```{r}
trainig_error <- mean(hemophilia$gr != predicciones$class) * 100
paste("trainig_error=", trainig_error, "%")
```


>>>>>>> Stashed changes

